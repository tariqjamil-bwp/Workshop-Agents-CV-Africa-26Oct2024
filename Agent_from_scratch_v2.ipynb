{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGENTS from scratch using Tool Calling / binding LLMS (NEW TRENDS)\n",
    "\n",
    "(Code for Computer Vision Africa - Workshop)\n",
    "\n",
    "Author: Tariq Jamil\n",
    "\n",
    "(TensorDot Solutions - www.tensordotsolutions.com)\n",
    "\n",
    "Linkedin: https://www.linkedin.com/in/tj-yazman/\n",
    "\n",
    "Dated: 26. October, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import re\n",
    "import json\n",
    "\n",
    "from Tools_r3 import calculate, currency_converter, get_news, ddg_search, get_weather\n",
    "from Tools_r3 import get_tool_specifications, cprint\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from typing import List, Union, Optional, Dict, Any\n",
    "\n",
    "class ChatClient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        base_url: str = \"https://api.groq.com/openai/v1\",\n",
    "        model: str = \"llama-3.1-70b-versatile\",\n",
    "        temperature: float = 0.0,\n",
    "        max_tokens: int = 512,\n",
    "        stream: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializing the chat client with default settings.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.stream = stream\n",
    "        self.tools = []  #  tools as an empty list in the begining\n",
    "\n",
    "        # Initializing OpenAI client\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=self.base_url,\n",
    "            api_key=self.api_key,\n",
    "        )\n",
    "\n",
    "    def bind_tools(self, tools: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Binding tools to the chat client.\n",
    "        \"\"\"\n",
    "        self.tools = tools\n",
    "\n",
    "    def run(self, message: Union[str, List[Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Running the chat client with specified message(s), returning the assistant's response.\n",
    "        \"\"\"\n",
    "        # This is to facilitate single query input, for testing etc.\n",
    "        if isinstance(message, str):\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"you are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": message}\n",
    "            ]\n",
    "        else:\n",
    "            messages = message\n",
    "\n",
    "        # Preparing the API call parameters\n",
    "        params = {\n",
    "            \"messages\": messages,\n",
    "            \"model\": self.model,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": self.max_tokens,\n",
    "            \"stream\": self.stream,\n",
    "        }\n",
    "\n",
    "        if self.tools:  # Including tools only if bind_tool use to add tools\n",
    "            params[\"tools\"] = self.tools\n",
    "            params[\"tool_choice\"] = 'auto'  # Set tool_choice as needed\n",
    "            \n",
    "        # Calling client to generate response\n",
    "        chat_completion = self.client.chat.completions.create(**params)\n",
    "\n",
    "        # Returning assistant's response\n",
    "        return chat_completion.choices[0].message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The system prompt can be broken down into 3 portions for understanfing sake.\n",
    "1. System prompt -- persona, system_message (ReAct methodology)\n",
    "2. Tools Ingestion (Tools description, and parameters they take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Assistant\"\n",
    "role = \"A general purpose assistant capable of answering user questions\"\n",
    "\n",
    "system_prompt_template = \"\"\"\n",
    "You are an intelligent '{{ name }}' and you act as '{{ role }}':\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Agents we need to maintain a state of messages from begining till last until answer is evaluated.\n",
    "\n",
    "The Chat Models has a dictionary style prompt, have 'role' keys and 'content' keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STATE & Other Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = Template(system_prompt_template).render(name=name, role=role)  # tools not specified\n",
    "# initializations\n",
    "max_iterations = 20\n",
    "\n",
    "# Initializing state\n",
    "messages_state = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "#user_query = \"What are top 5 news from capital of Nigeria. Also sum the numbers in '245323'\"\n",
    "user_query = \"What is the current temperature (celius) at capital of Nigeria.\"\n",
    "user_query1 = \"What is rain condition Lagos, with time and date.\"\n",
    "user_query2 = \"What is capital of Nigeria and 3 major cities?, in bullets form\"\n",
    "user_query3 = \"what is todays top 4 news items in Nigeria, in bullets form\"\n",
    "user_query4 = \"What are 3 best supermarkets in Abuja, in bullets form\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://platform.openai.com/docs/guides/function-calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the schemas\n",
    "from tool_schemas import (\n",
    "    tool_schema_get_news,\n",
    "    tool_schema_ddg_search,\n",
    "    tool_schema_get_weather,\n",
    "    tool_schema_calculate\n",
    ")\n",
    "\n",
    "# Now you can use the schemas in main.py\n",
    "#print(tool_schema_get_news)\n",
    "tools_spec = [\n",
    "    tool_schema_get_news,\n",
    "    tool_schema_ddg_search,\n",
    "    tool_schema_get_weather,\n",
    "    tool_schema_calculate] \n",
    "\n",
    "available_functions = {'get_news': get_news, 'ddg_search': ddg_search, 'get_weather': get_weather, \"calculate\": calculate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query3 = \"what is todays top 4 news items in Nigeria, in bullets form\"\n",
    "user_query = 'What is the current temperature (Celsius) at the capital of Nigeria, what is 2*3/80.'\n",
    "user_query3 = 'weather in Abuja, give buleted response, with current time and date'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> get_weather Tool Called --\n",
      "\n",
      "*\n",
      "***\n",
      "* Current Time: 16:52\n",
      "* Current Date: 2024-10-26\n",
      "* Location: Abuja, Nigeria\n",
      "* Temperature: 30.7°C (87.3°F)\n",
      "* Condition: Patchy rain nearby\n",
      "* Wind Speed: 5.1 mph (8.3 km/h)\n",
      "* Humidity: 57%\n",
      "* Cloud Cover: 74%\n",
      "* Feels Like: 34.0°C (93.2°F)\n",
      "* Dew Point: 21.3°C (70.4°F)\n",
      "* Air Quality: Moderate (US-EPA Index: 3, GB-Defra Index: 6)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "client = ChatClient()\n",
    "client.bind_tools(tools_spec) # registering tools with the client\n",
    "\n",
    "# Now using the function in your existing workflow\n",
    "messages_state.append({\"role\": \"user\", \"content\": user_query3})\n",
    "# Making the initial request\n",
    "response_message = client.run(messages_state)\n",
    "\n",
    "tool_calls = response_message.tool_calls\n",
    "#print(tool_calls)\n",
    "# Processing tool calls\n",
    "messages_state.append(response_message)\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    function_name = tool_call.function.name\n",
    "    function_to_call = available_functions[function_name]  # Ensure available_functions is defined\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    \n",
    "    # Calling the function with arguments\n",
    "    function_response = function_to_call(**function_args)\n",
    "    #print(function_response)\n",
    "\n",
    "    messages_state.append(\n",
    "        {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": str(function_response),\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Making the final request with tool call results\n",
    "final_response = client.run(messages_state)\n",
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class Agent:\n",
    "    def __init__(self, system_prompt: str, llm_client: ChatClient, available_functions: dict):\n",
    "        self.client = llm_client\n",
    "        self.available_functions = available_functions  # Available functions for tool calls\n",
    "        self.messages_state = [{\"role\": \"system\", \"content\": system_prompt}]  # Initialize with system prompt\n",
    "\n",
    "    def run(self, user_query: str):\n",
    "        # Appending the user query to the message state\n",
    "        self.messages_state.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "        # Making the initial request\n",
    "        response_message = self.client.run(self.messages_state)\n",
    "        print(response_message)\n",
    "        tool_calls = response_message.tool_calls\n",
    "\n",
    "        # Appending the response message to the message state\n",
    "        self.messages_state.append(response_message)\n",
    "\n",
    "        if tool_calls:\n",
    "            # Processing tool calls\n",
    "            for tool_call in tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_to_call = self.available_functions[function_name]  # Ensure available_functions is defined\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                # Calling the function with arguments\n",
    "                function_response = function_to_call(**function_args)\n",
    "\n",
    "                # Updating the message state with the tool call results\n",
    "                self.messages_state.append(\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"content\": str(function_response),\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            print(\"No tool calls found.\")\n",
    "        # Making the final request with tool call results\n",
    "        final_response = self.client.run(self.messages_state)\n",
    "        return final_response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='<function=get_weather>{\"location\": \"Abuja\"}<function>', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n",
      "No tool calls found.\n"
     ]
    }
   ],
   "source": [
    "llama3 = ChatClient()\n",
    "llama3.bind_tools(tools_spec)\n",
    "\n",
    "myAgent = Agent(system_prompt=system_prompt, llm_client=llama3, available_functions=available_functions)\n",
    "response = myAgent.run('Weather in Abuja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
